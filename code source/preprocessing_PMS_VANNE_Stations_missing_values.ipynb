{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ccb540",
   "metadata": {},
   "source": [
    "### Prediction des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088877d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PMS1 dataset created: 8353 rows\n",
      "âœ… VANNE dataset created: 8353 rows\n",
      "âœ… PMS2 dataset created: 8353 rows\n",
      "âœ… PMS3 dataset created: 8353 rows\n",
      "âœ… PMS4 dataset created: 8353 rows\n",
      "ðŸŽ¯ All PMS/Vanne datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Load datasets\n",
    "# -----------------------------\n",
    "head_df = pd.read_csv('./new_datasets/head_dataset.csv', sep=',', decimal='.')\n",
    "# head_df['Date'] = pd.to_datetime(head_df['Date'], dayfirst=True)\n",
    "\n",
    "pms_df = pd.read_csv('./new_datasets/Cleaned_PMS_SV_station.csv', sep=',', decimal='.')\n",
    "# pms_df['Date'] = pd.to_datetime(pms_df['Date'], dayfirst=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Mapping PMS/Vanne stations to their pressure columns\n",
    "# -----------------------------\n",
    "stations = {\n",
    "    'PMS1': '11-PIT-001',\n",
    "    'VANNE': 'Presure_SV_Average',\n",
    "    'PMS2': '13-PIT-001',\n",
    "    'PMS3': '14-PIT-001',\n",
    "    'PMS4': '15-PIT-001'\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Create dataset per station\n",
    "# -----------------------------\n",
    "for station, pressure_col in stations.items():\n",
    "    df = pd.DataFrame()\n",
    "    df['Date'] = pms_df['Date']  # keep Date from PMS dataset\n",
    "    df[f'Pressure_{station}'] = pms_df[pressure_col]\n",
    "    df[f'Flow_{station}'] = head_df['Flow_HS']  # same flow as head station\n",
    "    df[f'Density_{station}'] = head_df['Density_HS_Average']  # same density as head station for now\n",
    "\n",
    "    df.to_csv(\"./new_datasets/\" + f'{station}_dataset.csv', index=False, sep=',', decimal='.')\n",
    "    print(f\"âœ… {station} dataset created: {df.shape[0]} rows\")\n",
    "\n",
    "print(\"ðŸŽ¯ All PMS/Vanne datasets created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "864a89ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PMS1 predictions saved (sans Head station).\n",
      "âœ… PMS2 predictions saved (sans Head station).\n",
      "âœ… PMS3 predictions saved (sans Head station).\n",
      "âœ… PMS4 predictions saved (sans Head station).\n",
      "âœ… VANNE predictions saved (sans Head station).\n",
      "ðŸŽ¯ All PMS/VANNE stations processed with Head station data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load Head station data\n",
    "head_df = pd.read_csv('./new_datasets/head_dataset.csv')\n",
    "\n",
    "# List of PMS/VANNE stations and their pressure columns\n",
    "stations = {\n",
    "    'PMS1': '11-PIT-001',\n",
    "    'PMS2': '13-PIT-001',\n",
    "    'PMS3': '14-PIT-001',\n",
    "    'PMS4': '15-PIT-001',\n",
    "    'VANNE': 'Presure_SV_Average'\n",
    "}\n",
    "\n",
    "for station, pressure_col in stations.items():\n",
    "    # Load PMS/VANNE station data\n",
    "    pms_df = pd.read_csv(f'./new_datasets/{station}_dataset.csv')\n",
    "    # Merge with Head station on Date\n",
    "    merged = pd.merge(pms_df, head_df, on='Date', suffixes=(f'_{station}', '_Head'))\n",
    "    # Features: Pressure at PMS/VANNE + Flow and Density at Head\n",
    "    X = merged[[f'Pressure_{station}', 'Flow_HS', 'Density_HS_Average']]\n",
    "    # Targets: Flow and Density at PMS/VANNE\n",
    "    y_flow = merged[f'Flow_{station}']\n",
    "    y_density = merged[f'Density_{station}']\n",
    "    # Train/test split\n",
    "    X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X, y_flow, test_size=0.2, random_state=42)\n",
    "    X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X, y_density, test_size=0.2, random_state=42)\n",
    "    # Train models\n",
    "    model_flow = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model_flow.fit(X_train_f, y_train_f)\n",
    "    model_density = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model_density.fit(X_train_d, y_train_d)\n",
    "    # Predict for all rows\n",
    "    merged[f'Flow_{station}'] = model_flow.predict(X)\n",
    "    merged[f'Density_{station}'] = model_density.predict(X)\n",
    "    # Save only Date, Pressure, Flow_pred, Density_pred for the station\n",
    "    output = merged[['Date', f'Pressure_{station}', f'Flow_{station}', f'Density_{station}']]\n",
    "    output.to_csv(f'./new_dataset/updated_{station}_pred.csv', index=False)\n",
    "    print(f'âœ… {station} predictions saved (sans Head station).')\n",
    "print('ðŸŽ¯ All PMS/VANNE stations processed with Head station data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc428a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged dataset created: 8353 rows, 22 columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "from functools import reduce\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# Merge all the 7 updated datasets into one cleaned dataframe\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "stations = ['Head', 'PMS1', 'Vanne', 'PMS2', 'PMS3', 'PMS4', 'Terminal']\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# Load updated datasets\n",
    "for station in stations:\n",
    "    if station in ['Head', 'Terminal']:\n",
    "        dfs[station] = pd.read_csv(f'./new_datasets/{station}_dataset.csv', sep=',', decimal='.')\n",
    "    else:\n",
    "        dfs[station] = pd.read_csv(f'./new_dataset/updated_{station}_pred.csv', sep=',', decimal='.')\n",
    "# Merge all datasets on 'Date'\n",
    "dfs_list = [dfs[station] for station in stations]\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='Date', how='outer'), dfs_list)\n",
    "\n",
    "# Save merged dataset\n",
    "merged_df.to_csv('./new_dataset/Merged_All_Stations_updated.csv', index=False, sep=',', decimal='.')\n",
    "print(f\"âœ… Merged dataset created: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
